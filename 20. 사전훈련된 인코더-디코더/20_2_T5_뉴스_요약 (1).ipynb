{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) 데이터 다운로드"
      ],
      "metadata": {
        "id": "TDx8R7_N9JCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEtQxs2E9DSP",
        "outputId": "76ebe77c-ebc8-460f-d2fb-dd0e9b66f117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 다운로드\n",
        "!gdown https://drive.google.com/uc?id=13l621lx2nSnXpFpzh78UUEyds_DAzyn6\n",
        " # 테스트 데이터 다운로드\n",
        "!gdown https://drive.google.com/uc?id=10LwhiPlgjOZbtF0Bv5395wYIm23y_QfT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI67NJWG9SjT",
        "outputId": "221a64bf-2e1d-4b3c-891c-c8ad5fffb8cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13l621lx2nSnXpFpzh78UUEyds_DAzyn6\n",
            "From (redirected): https://drive.google.com/uc?id=13l621lx2nSnXpFpzh78UUEyds_DAzyn6&confirm=t&uuid=b54c0671-a5c9-4240-ae94-fa707ae4cb8d\n",
            "To: /content/summ_train.json\n",
            "100% 1.16G/1.16G [00:11<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10LwhiPlgjOZbtF0Bv5395wYIm23y_QfT\n",
            "From (redirected): https://drive.google.com/uc?id=10LwhiPlgjOZbtF0Bv5395wYIm23y_QfT&confirm=t&uuid=e446619d-b7b1-4faa-b738-0482af547b6a\n",
            "To: /content/summ_test.json\n",
            "100% 147M/147M [00:01<00:00, 99.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "DATA_TRAIN_PATH = 'summ_train.json'\n",
        "train_df = pd.read_json(DATA_TRAIN_PATH)\n",
        "train_df = train_df.dropna()\n",
        "train_df = train_df[:4000]\n",
        "print('학 습 데이터의 개수:', len(train_df))\n",
        "DATA_TEST_PATH = 'summ_test.json'\n",
        "test_df = pd.read_json(DATA_TEST_PATH)\n",
        "test_df = test_df.dropna()\n",
        "test_df = test_df[:200]\n",
        "print('테 스트 데이터의 개수:', len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DSDIzlE9R9f",
        "outputId": "a0e4737f-4a6e-44be-c2ea-c558e2ade619"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학 습 데이터의 개수: 4000\n",
            "테 스트 데이터의 개수: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#documents 파싱\n",
        "def preprocess_data(data):\n",
        "    outs = []\n",
        "    for doc in data['documents']:\n",
        "        line = []\n",
        "        line.append(doc['media_name'])\n",
        "        line.append(doc['id'])\n",
        "        para = []\n",
        "        for sent in doc['text']:\n",
        "            for s in sent:\n",
        "                para.append(s['sentence'])\n",
        "        line.append(para)\n",
        "        line.append(doc['abstractive'][0])\n",
        "        line.append(doc['extractive'])\n",
        "        a = doc['extractive']\n",
        "        if a[0] == None or a[1] == None or a[2] == None:\n",
        "            continue\n",
        "        outs.append(line)\n",
        "    outs_df = pd.DataFrame(outs, columns = ['media', 'id', 'article_original', 'abstractive', 'extractive'])\n",
        "    return outs_df"
      ],
      "metadata": {
        "id": "XiggsyqW9tuQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = preprocess_data(train_df)\n",
        "test_data = preprocess_data(test_df)"
      ],
      "metadata": {
        "id": "4KMek-at-jDR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['news'] = train_data['article_original'].apply(lambda x : ' '.join(x))\n",
        "test_data['news'] = test_data['article_original'].apply(lambda x : ' '.join(x))"
      ],
      "metadata": {
        "id": "uJ8EOOS1-r-_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 정수 인코딩"
      ],
      "metadata": {
        "id": "1CLaovOk--TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   입력, 레이블  정수 인코딩\n",
        "2.   입력에 'summarize' : 2200, ':' : 153을 맨 앞에 추가\n",
        "3.   입력과 레이블에 < /s >: 1 을 각각 맨 끝에 추가\n",
        "4.   레이블에 패딩 토큰 -100을 추가\n",
        "5.   인코더의 입력이 인코더로, 모델은 디코더의 레이블 예측\n"
      ],
      "metadata": {
        "id": "z2vkNIhP_P-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) 정수 인코딩을 위한 DATASET 생성\n"
      ],
      "metadata": {
        "id": "vlUh6UjnAAQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFT5ForConditionalGeneration, T5TokenizerFast\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jEcMWLMg-83Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class T5SummaryDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, tokenizer, max_len, batch_size, ignore_index = -100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.docs = df\n",
        "        self.batch_size = batch_size\n",
        "        self.ignore_index = ignore_index\n",
        "        self.indices = list(range(len(self.docs)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch = self.docs.iloc[batch_indices]\n",
        "\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "\n",
        "        for _, instance in batch.iterrows():\n",
        "            encoded_input = self.tokenizer.encode(\"summarize:\" + instance['news'], max_length = self.max_len, padding = 'max_length', truncation = True)\n",
        "            input_ids.append(encoded_input)\n",
        "\n",
        "            encoded_label = self.tokenizer.encode(instance['abstractive'], max_length = self.max_len, padding = 'max_length', truncation = True)\n",
        "            label = [l if l != self.tokenizer.pad_token_id else self.ignore_index for l in encoded_label]\n",
        "            labels.append(label)\n",
        "\n",
        "        #배치 데이터 반환\n",
        "        return {\n",
        "            'input_ids' : np.array(input_ids),\n",
        "            'labels': np.array(labels)\n",
        "        }\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)"
      ],
      "metadata": {
        "id": "RtLANaKVAcZH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) 모델 클래스 선언"
      ],
      "metadata": {
        "id": "sSzJnfcaCmPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFT5ForConditionalGeneration.from_pretrained('paust/pko-t5-base' , from_pt = True)\n",
        "tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11JtEQJXAcgE",
        "outputId": "008f7804-5fcf-4464-84d8-6fc6d8ed8b7e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
            "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) 데이터로더 변환"
      ],
      "metadata": {
        "id": "dSg9_BrNC4LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =8\n",
        "max_len = 256\n",
        "lr = 3e-5\n",
        "max_epochs = 5\n",
        "warmup_ratio = 0.1"
      ],
      "metadata": {
        "id": "xJ6rx0KAAckn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = T5SummaryDataset(train_data, tokenizer, max_len = max_len, batch_size = batch_size)\n",
        "test_dataset = T5SummaryDataset(test_data, tokenizer, max_len = max_len, batch_size = batch_size)\n",
        "\n",
        "total_steps = len(train_dataset) * max_epochs\n",
        "warmup_steps = int(total_steps * warmup_ratio)\n",
        "lr_schedule = CosineDecay(initial_learning_rate = lr, decay_steps = total_steps)\n",
        "optimizer = Adam(learning_rate = lr_schedule)"
      ],
      "metadata": {
        "id": "Cvreak7w-8z3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUrjk2gyD5AD",
        "outputId": "3ea456d0-2d3a-4b0a-db22-e4df07f734f6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) 데이터 확인"
      ],
      "metadata": {
        "id": "agbtn6xR9JvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"첫번째 샘플의 원문 텍스트:\", train_data['news'].loc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU2bNd_MEA01",
        "outputId": "a622a0cd-8145-4e11-b82e-6cc5d869fe93"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 원문 텍스트: ha당 조사료 400만원…작물별 차등 지원 이성훈 sinawi@hanmail.net 전라남도가 쌀 과잉문제를 근본적으로 해결하기 위해 올해부터 시행하는 쌀 생산조정제를 적극 추진키로 했다. 쌀 생산조정제는 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 제도다. 올해 전남의 논 다른 작물 재배 계획면적은 전국 5만ha의 약 21%인 1만 698ha로, 세부시행지침을 확정, 시군에 통보했다. 지원사업 대상은 2017년산 쌀 변동직불금을 받은 농지에 10a(300평) 이상 벼 이외 다른 작물을 재배한 농업인이다. 지원 대상 작물은 1년생을 포함한 다년생의 모든 작물이 해당되나 재배 면적 확대 시 수급과잉이 우려되는 고추, 무, 배추, 인삼, 대파 등 수급 불안 품목은 제외된다. 농지의 경우도 이미 다른 작물 재배 의무가 부여된 간척지, 정부매입비축농지, 농진청 시범사업, 경관보전 직불금 수령 농지 등은 제외될 예정이다. ha(3000평)당 지원 단가는 평균 340만원으로 사료작물 400만원, 일반작물은 340만원, 콩·팥 등 두류작물은 280만원 등이다. 벼와 소득차와 영농 편이성을 감안해 작물별로 차등 지원된다. 논에 다른 작물 재배를 바라는 농가는 오는 22일부터 2월 28일까지 농지 소재지 읍면동사무소에 신청해야 한다. 전남도는 도와 시군에 관련 기관과 농가 등이 참여하는‘논 타작물 지원사업 추진협의회’를 구성, 지역 특성에 맞는 작목 선정 및 사업 심의 등을 본격 추진할 방침이다. 최향철 전라남도 친환경농업과장은 “최근 쌀값이 다소 상승추세에 있으나 매년 공급과잉에 따른 가격 하락으로 쌀농가에 어려움이 있었다”며“쌀 공급과잉을 구조적으로 해결하도록 논 타작물 재배 지원사업에 많이 참여해주길 바란다”고 말했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"정수 인코딩 및 패딩 결과:\", train_dataset[0]['input_ids'][0])\n",
        "print(len(train_dataset[0]['input_ids'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6eZSNs_EHXF",
        "outputId": "bc2ccac9-8bb1-4563-c6af-9c8a032941f8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 및 패딩 결과: [ 7675    78 20359    74 26159    27  7231   480   222  1526   541   222\n",
            "  7004    17 23431    15    15    15 13238   681   222 18165   222   926\n",
            "   222  5009  1303   222    84  2667    66  7066    33    73  3415 11261\n",
            " 11866    15 39816   222  5809  6171   278   222  2533   222 12050   871\n",
            "   333   222  6778   403   373   222  1745   701   222   863   222  1387\n",
            "   667   222  2642   429   222  2533   222  1951  2487   354   333   222\n",
            "  2430   222  1808  4210   222   500    15   222  2533   222  1951  2487\n",
            "   354   274   222  1650   333   222   527  1963   222  1283   279   222\n",
            "  1650   222  2233   222  4758 13238   824   222  1967   222   450   222\n",
            "   804   222 13238   291   222 49511   222  1650  2911   222  1928   222\n",
            "  2470   466   333   222   336  3502  1116   222  2452   267    15   222\n",
            "  1387   222  4147   302   222  1283   222   804   222 13238   222  5431\n",
            "   222  1247  4107   311   222  1404   222    22   348  7231   302   222\n",
            "   585   222  3738     6   321   222    18   348   222    23    26    25\n",
            "  7231   293    13   222  4850  2642  9031   291   222  2311    13   222\n",
            " 10512   279   222  2376   500    15   222   926   935   222  1168   311\n",
            "   222  2291  3634   482   473   222  2533   222  4071   519 13325   291\n",
            "   222  1669   222 14205   279   222  1477    66     9  2534    17   671\n",
            "    10   222   811   222  1650   222  4099   222   804   222 13238   291\n",
            "   222  5431   305   222  4041   321   570    15   222   926   222  1168\n",
            "   222 13238   311   222    18 28367   291   222  1291   305   222   267\n",
            " 28367   302   222     1]\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"첫번째 샘플의 정수 인코딩 후 복원 결과:\", tokenizer.decode(train_dataset[0]['input_ids'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO7f5hSzEHZ3",
        "outputId": "8a0d0c1d-0d68-41f7-e566-0a8f32f47483"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 정수 인코딩 후 복원 결과: summarize:ha당 조사료 400만원...작물별 차등 지원 이성훈 sinawi@hanmail.net 전라남도가 쌀 과잉문제를 근본적으로 해결하기 위해 올해부터 시행하는 쌀 생산조정제를 적극 추진키로 했다. 쌀 생산조정제는 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 제도다. 올해 전남의 논 다른 작물 재배 계획면적은 전국 5만ha의 약 21%인 1만 698ha로, 세부시행지침을 확정, 시군에 통보했다. 지원사업 대상은 2017년산 쌀 변동직불금을 받은 농지에 10a(300평) 이상 벼 이외 다른 작물을 재배한 농업인이다. 지원 대상 작물은 1년생을 포함한 다년생의 </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"첫번째 샘플의 요약문:\" , train_data['abstractive'].loc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9txp-_nEHdF",
        "outputId": "bbef0a3f-1532-4ed4-a35b-e92086c83fcf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 요약문: 전라남도가 쌀 과잉문제를 근본적으로 해결하기 위해 올해부터 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 '쌀 생산조정제'를 적극적으로 시행하기로 하고 오는 22일부터 2월 28일까지 농지 소재지 읍면동사무소에서 신청받는다 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"첫번째 샘플의 요약문의 정수 인코딩 및 패딩 결과:\", train_dataset[0]['labels'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiraXPT0EqI6",
        "outputId": "a93e35d0-878b-410e-96b0-b415201305c1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 요약문의 정수 인코딩 및 패딩 결과: [ 5809  6171   278   222  2533   222 12050   871   333   222  6778   403\n",
            "   373   222  1745   701   222   863   222  1387   667   222  1650   333\n",
            "   222   527  1963   222  1283   279   222  1650   222  2233   222  4758\n",
            " 13238   824   222  1967   222   450   222   804   222 13238   291   222\n",
            " 49511   222  1650  2911   222  1928   222  2470   466   333   222   336\n",
            "  3502  1116   222     8  2533   222  1951  2487   354     8   333   222\n",
            "  2430   403   373   222  2642   701   293   222   443   222  1243   222\n",
            "  3858   349   667   222    19   515   222  3732   349   579   222 14205\n",
            "   222  2500   284   222 30492  1588   402   389   222  1143  6604   222\n",
            "    15     1  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100\n",
            "  -100  -100  -100  -100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_array = train_dataset[0]['labels'][0]\n",
        "test_array[test_array == -100] = 0\n",
        "\n",
        "print(\"첫번째 샘플의 요약문 레이블:\", tokenizer.decode(test_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRyOeJjEqFv",
        "outputId": "c52abe15-21cf-4926-bae4-5f878f1bfaf6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 요약문 레이블: 전라남도가 쌀 과잉문제를 근본적으로 해결하기 위해 올해부터 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 '쌀 생산조정제'를 적극적으로 시행하기로 하고 오는 22일부터 2월 28일까지 농지 소재지 읍면동사무소에서 신청받는다 .</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) 어텐션 마스크 함수"
      ],
      "metadata": {
        "id": "H73WyA9_FC91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_attention_mask(input_ids):\n",
        "    boolean_mask = tf.not_equal(input_ids, tokenizer.pad_token_id)\n",
        "    return tf.cast(boolean_mask, tf.float32)"
      ],
      "metadata": {
        "id": "siND0X1gFEng"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) 학습"
      ],
      "metadata": {
        "id": "8AprsSznI_7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 루프\n",
        "best_loss = float('inf')\n",
        "\n",
        "#총 훈련할 에폭수 설정\n",
        "max_epochs = 5\n",
        "for epoch in range(max_epochs):\n",
        "    print(f'에포크 {epoch+1}/ {max_epochs}')\n",
        "\n",
        "    #훈련\n",
        "    total_train_loss = 0.0\n",
        "    for batch in tqdm(train_dataset, total = len(train_dataset), desc = '훈련중'):\n",
        "        attention_mask = create_attention_mask(batch['input_ids'])\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(input_ids = batch['input_ids'],\n",
        "                            attention_mask = attention_mask,\n",
        "                            labels = batch['labels'],\n",
        "                            training = True)\n",
        "            loss = outputs.loss\n",
        "        total_train_loss += tf.reduce_mean(loss).numpy() #배치의 평균손실을 누적\n",
        "        #그레디언트 계산\n",
        "        gradients =tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    #에폭의 평균 훈련 손실 계산\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "    print(f'훈련 손실 : {avg_train_loss: .4f}')\n",
        "\n",
        "    #평가\n",
        "    total_val_loss = 0.0\n",
        "    for batch in tqdm(test_dataset, total = len(test_dataset), desc = '검증 중'):\n",
        "        attention_mask = create_attention_mask(batch['input_ids'])\n",
        "        outputs = model(input_ids = batch['input_ids'],\n",
        "                        attention_mask = attention_mask,\n",
        "                        labels = batch['labels'],\n",
        "                        training = False)\n",
        "        total_val_loss += tf.reduce_mean(outputs.loss).numpy()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_dataset)\n",
        "    print(f\"검증 손실 :  {avg_val_loss: .4f}\")\n",
        "\n",
        "    #최고 성능 모델 저장\n",
        "    #현재 에폭의 검증 손실이 이전 최저 손실보다 낮은 경우\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        model.save_pretrained('model')\n",
        "        print(f\"검증 손실이 {best_loss: .4f}로 개선\")\n",
        "\n",
        "    #에포크 종료 시 데이터 셔플\n",
        "    train_dataset.on_epoch_end()\n",
        "\n",
        "print('훈련이 완료되었습니다.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ksCpj8DJBpE",
        "outputId": "5546c18d-5342-4b43-f539-d8399b217eb2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 1/ 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "훈련중: 100%|██████████| 500/500 [21:31<00:00,  2.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실 :  1.3483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "검증 중: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 손실 :   1.3247\n",
            "검증 손실이  1.3247로 개선\n",
            "에포크 2/ 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "훈련중: 100%|██████████| 500/500 [21:30<00:00,  2.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실 :  1.1402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "검증 중: 100%|██████████| 25/25 [00:13<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 손실 :   1.2861\n",
            "검증 손실이  1.2861로 개선\n",
            "에포크 3/ 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "훈련중: 100%|██████████| 500/500 [21:29<00:00,  2.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실 :  1.0788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "검증 중: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 손실 :   1.2842\n",
            "검증 손실이  1.2842로 개선\n",
            "에포크 4/ 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "훈련중: 100%|██████████| 500/500 [21:27<00:00,  2.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실 :  1.0453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "검증 중: 100%|██████████| 25/25 [00:13<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 손실 :   1.2860\n",
            "에포크 5/ 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "훈련중: 100%|██████████| 500/500 [21:29<00:00,  2.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실 :  1.0302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "검증 중: 100%|██████████| 25/25 [00:13<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 손실 :   1.2867\n",
            "훈련이 완료되었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9) 로드 및 평가"
      ],
      "metadata": {
        "id": "llI-iUObNS0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = TFT5ForConditionalGeneration.from_pretrained('model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjs2iBGSM3Wu",
        "outputId": "0dcc6468-dac2-48c3-f7cf-ae90f66b7092"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_datset):\n",
        "    total_val_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(test_dataset, total = len(test_dataset), desc = '평가 중'):\n",
        "        attention_mask = create_attention_mask(batch['input_ids'])\n",
        "        outputs = model(input_ids = batch['input_ids'],\n",
        "                        attention_mask = attention_mask,\n",
        "                        labels = batch['labels'],\n",
        "                        training = False)\n",
        "        total_val_loss += tf.reduce_mean(outputs.loss).numpy()\n",
        "    avg_val_loss = total_val_loss / len(test_dataset)\n",
        "\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "uJEgRjPEN4O4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate_model(loaded_model,  test_dataset)\n",
        "print(f\"테스트 손실: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyL-ReWrOegY",
        "outputId": "64d578e3-92b1-49b0-b5ba-5064d4bc1a7b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "평가 중: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 손실: 1.2842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  10) 요약문 생성"
      ],
      "metadata": {
        "id": "sy8GQ65-OpbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, model, tokenizer, max_length = 300):\n",
        "    input_text = 'summarize' + text\n",
        "    inputs = tokenizer(input_text, return_tensors = \"tf\", max_length = 512, truncation = True, padding = 'max_length')\n",
        "\n",
        "    attention_mask = create_attention_mask(inputs['input_ids'])\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        input_ids = inputs['input_ids'],\n",
        "        attention_mask = attention_mask,\n",
        "        max_length = max_length,\n",
        "        num_beams = 7,\n",
        "        repetition_penalty = 2.0\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "WbkvZztCOuJk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = test_data.loc[0]['news']\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2OZrXZFPl3D",
        "outputId": "80a4a1dc-f3e4-47d0-84d3-fe6bcd5a27ab"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 박재원 기자 ] '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 \"넓고, 체증 없는 '통신 고속도로'가 5G\"라며 \"대한민국의 대전환이 이제 막 시작됐다\"고 기대감을 높였다. 문 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G 시대는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\"라며 \"5G는 대한민국 혁신성장의 인프라\"라고 강조했다. 산업화 시대에 고속도로가 우리 경제의 '대동맥' 역할을 했듯, 5G가 4차 산업혁명 시대의 고속도로가 돼 새로운 기회를 열어 줄 것이란 설명이다. 문 대통령은 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론(무인항공기), 로봇, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\"고 밝혔다. 세계 최초 상용화에 성공한 5G가 반도체를 이을 우리 경제의 새 먹거리가 될 것이란 관측이다. 정부는 2026년 세계 5G 시장 규모가 1161조원에 달할 것으로 보고 있다. 작년 반도체 시장 규모가 529조원인 점을 고려하면 2배 이상 큰 미래 시장이 창출되는 셈이다. 문 대통령은 아직은 국민에게 다소 낯선 5G 시대의 미래상을 친절히 설명해 눈길을 끌기도 했다. 문 대통령은 \"'지금 스마트폰으로 충분한데, 5G가 왜 필요하지?'라고 생각할 수 있다\"며 \"4세대 이동통신은 '아직은' 빠르지만 가까운 미래에는 결코 빠르지 않다\"고 했다. 그러면서 \"자동차가 많아질수록 더 넓은 길이 필요한 것처럼 사물과 사물을 연결하고, 데이터를 주고받는 이동통신망도 더 넓고 빠른 길이 필요하다\"고 덧붙였다. 문 대통령은 세계 최초 상용화에 성공한 우리 5G 기술을 널리 알리는 홍보대사를 자처하기도 했다. 5G 시장을 선점하기 위한 각국의 경쟁이 뜨겁게 달아오른 만큼 정부 차원에서 적극 지원하겠다는 방침이다. 문 대통령은 \"평창동계올림픽 360도 중계, 작년 4·27 남북한 정상회담 때 프레스센터에서 사용된 스마트월처럼 기회가 생기면 대통령부터 나서서 우리의 앞선 기술을 홍보하겠다\"고 말했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(text, model, tokenizer)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KovZpRklPeVH",
        "outputId": "7071ce52-b657-478e-8bef-2cbca90662d7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문재인 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\"라며 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\"고 말했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "osVKOeHZPvsh"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}